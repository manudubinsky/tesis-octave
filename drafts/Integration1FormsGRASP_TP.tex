\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}

\usepackage[table,xcdraw]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{ulem}
\usepackage{color}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage {tikz}
\usetikzlibrary{arrows,matrix,positioning,fit}
%\usepackage{bera}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{framed}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
%\setenumerate[2]{label=\roman*.}
\usepackage{blindtext}
\usepackage{fancyhdr}
\usepackage{bm}
\pagestyle{fancy}
\usepackage{hhline}
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor {processblue}{cmyk}{0.96,0,0,0}

%\setenumerate[1]{label=\textbf{\thesection.\arabic*.}}


\begin{document}
\textbf{Una Metaheurística GRASP para Integración No Orientada en Grafos}

\bigskip

\textbf{Abstract}
 
\bigskip

Dado un grafo conexo $G=(V,E)$ y una función de pesos en los nodos $x: V 
\rightarrow \mathbb{R}$, definimos el \textit{diferencial no orientado} 
$dx: E \rightarrow \mathbb{R}^+$ en cada eje $e=(v,w)$ como $dx(e) = 
|x(v) - x(w)|$.

\smallskip

En el presente trabajo abordamos el siguiente problema: dado un grafo 
$G=(V,E)$ y una función de pesos en los ejes $f: E \rightarrow 
\mathbb{R}^+$, se trata de encontrar la función de pesos en los nodos 
$x: V \rightarrow \mathbb{R}$ cuyo \textit{diferencial no orientado} 
ajuste del mejor modo posible (en términos de cuadrados mínimos) los 
pesos de los ejes dados por $f$.

\smallskip

Para ello presentamos un algoritmo basado en la metaheurística GRASP 
cuya simplicidad permite paralelizarlo para procesar instancias de 
tamaño arbitrario.

\bigskip

\textbf{Introducción}

\bigskip

En esta sección se presenta el problema que abordaremos en el trabajo. 
De acuerdo a nuestro entendimiento, se trata de una formulación 
novedosa, por lo tanto no hay antecedentes a los cuales podamos 
remitirnos.

\bigskip

Dado un grafo conexo $G=(V,E)$ y una función de pesos en los ejes 
$f: E \rightarrow \mathbb{R}^+$, queremos hallar una función de pesos 
en los nodos $x: V \rightarrow \mathbb{R}$ de modo de minimizar el 
error:

$$E(x) := \sum_{e \in E} (dx(e) - f(e))^2$$

donde $dx: E \rightarrow \mathbb{R}^+$ es el \textit{diferencial 
no orientado} de $x$ y se define en cada eje $e=(v,w)$ como $dx(e) = 
|x(v) - x(w)|$.

\bigskip

Por otro lado, es posible definir en un grafo dirigido una noción de 
\textit{diferencial orientado} del siguiente modo: dado un grafo 
dirigido $\vec G=(\vec V,\vec E)$ y una función de pesos en los nodos 
$\vec x: \vec V \rightarrow \mathbb{R}$, se define el 
\textit{diferencial orientado} $\vec{dx}: \vec E \rightarrow \mathbb{R}$ 
en cada eje $e=v \rightarrow w$ como $\vec{dx}(e) = \vec x(w) - 
\vec x(v)$.

\bigskip

Es importante notar que un \textit{diferencial no orientado} induce 
naturalmente un \textit{diferencial orientado}, informalmente: hay que 
orientar cada eje del nodo ``menor" al ``mayor". Recíprocamente, si
se analizan todas las orientaciones posibles del grafo original, es 
posible resolver el problema propuesto. Concretamente, se trata de 
encontrar por un lado la ``mejor" \textit{matriz de incidencia 
dirigida} $D$, y por el otro la mejor función de pesos en 
los ejes $x: V \rightarrow \mathbb{R}$ de modo de minimizar la 
expresión:

$$E(x) = ||D\bm{x}-\bm{f}||^2$$

donde:

$$
\bm{x} = 
\begin{bmatrix}
	x(v_1)\\
	\dots \\
	x(v_n)
\end{bmatrix}, 
\bm{f} = 
\begin{bmatrix}
	f(e_1)\\
	\dots \\
	f(e_m)
\end{bmatrix}
$$

\bigskip

El algoritmo que proponemos justamente explora el espacio de todas las 
orientaciones posibles del grafo original.

\bigskip

\newpage

\textbf{Aplicaciones}

\bigskip

Las aplicaciones son aquellas en las cuales necesitamos construir una 
función de los nodos que respete ciertas restricciones diferenciales.

\bigskip

Por ejemplo, supongamos que los nodos representan regiones geográficas 
y queremos construir un mapa topográfico basado en ciertas relaciones 
diferenciales de altura entre algunas regiones.

\bigskip

Otro ejemplo podría ser el de ordenar cronológicamente ciertos eventos 
en base a algunas relaciones diferenciales entre ellos.

\bigskip

\textbf{Preliminares}

\bigskip

\underline{La \textit{matriz de incidencia dirigida}}

\bigskip

La \textit{matriz de incidencia dirigida} ($D$) de un grafo dirigido 
$\vec G = (\vec V, \vec E)$ es una matriz de $m \times n$ (donde $|\vec
 E| = m$ y $|\vec V| = n$) tal que para cada eje orientado 
$e_k=v_i \rightarrow v_j$: $D_{ki} = -1$, $D_{kj} = 1$ y vale $0$ en 
las demás entradas de la fila $k$.

\bigskip

En otros términos, una matriz de incidencia dirigida representa una 
posible orientación de los ejes de un grafo no dirigido $G$. Y el 
espacio de matrices de incidencia dirigida asociadas a $G$ 
equivale al espacio de todas las orientaciones posibles de $G$.

\bigskip

\underline{La \textit{matriz laplaciana}}

\bigskip

La \textit{matriz laplaciana} ($L$) de un grafo $G$ es una matriz de 
$n \times n$ definida del siguiente modo:

\bigskip

\begin{equation}
	\begin{cases}
	deg(v_i) & \text{en $L_{ii}$} \\
	-1 & \text{en $L_{ij}$ si existe el eje $(v_i,v_j)$} \\
	0 & \text{en otro caso} 
	\end{cases}
\end{equation}

\bigskip

El rango de $L$ es $n-c$ donde $c$ es la cantidad de componentes 
conexas. En particular si $G$ es conexo, el rango de $L$ es $n-1$

\bigskip

\underline{Relación entre las matrices $L$ y $D$}

\bigskip

Es fácil verificar que las matrices $L$ y $D$ se relacionan de acuerdo
a la expresión:

$$L = D^t D$$

Es importante notar que $L$ es independiente de cualquier orientación 
que se le asocie al grafo. Y que el rango de $D$ es el mismo que el de 
$L$. Como en nuestro caso $G$ es conexo el rango de $D$ es $n-1$.

\bigskip

\underline{Sistema lineal inducido por $D$}

\bigskip

Recordemos que el problema que queremos resolver es encontrar la matriz 
de incidencia dirigida $D$, es decir la ``mejor" orientación de los ejes 
del grafo $G$, de modo de minimizar la expresión:

$$E(x) = ||D\bm{x}-\bm{f}||^2$$

Para eso será preciso explorar el espacio de matrices de 
incidencia dirigida asociadas a $G$. Una vez que conseguimos una 
matriz candidata, el problema de minimización es equivalente a encontrar
dónde se anula el gradiente de $E(x)$:
 
$$\nabla E = [\frac{\partial E}{\partial x_1}, \dots, \frac{\partial 
E}{\partial x_n}] = D^tDx-D^tv=0$$

Que a su vez es equivalente a resolver el sistema lineal:

$$D^tDx = Lx = D^tv$$

Como el rango de $L$ es $n-1$, el sistema puede no tener solución. 
Mediante métodos iterativos es posible converger al punto más cercano a 
una solución. Específicamente en este trabajo emplearemos el método de 
\textit{Gradiente Conjugado}. Es importante mencionar que en el último 
tiempo ha habido grandes avances en la creación de nuevos métodos 
iterativos para resolver esta clase particular de sistemas lineales 
(aquellos que involucran una matriz laplaciana) denominados 
\textit{sistemas laplacianos}.

\bigskip

\underline{Norma de la proyección ortogonal}

\bigskip

Para elaborar el criterio que nos permita comparar dos matrices de 
incidencia dirigida y decidir cuál es mejor en el contexto del problema 
que queremos resolver, vamos a referirnos a nociones básicas de 
proyección ortogonal de un vector sobre un subespacio de un espacio 
vectorial.

\bigskip

Sean $v,w \in \mathbb{R}^n$, notamos $p_v(w)$ a la proyección de $w$ 
sobre $v$. Mediante relaciones trigonométricas básicas podemos 
calcular $||p_v(w)||$ (la norma de $p_v(w)$):

$$\frac{v w}{||w||} = \frac{cos(\alpha) \ ||v|| \ ||w||}{||w||} = 
cos(\alpha) \ ||v|| = ||p_v(w)||$$

\bigskip

De modo más general, notamos $p_S(w)$ a la proyección ortogonal de $w$
sobre un subespacio $S \subset \mathbb{R}^n$ generado por una base 
ortogonal $S = <s_1, \dots, s_k>$. La norma de $p_S(w)$ puede calcularse
de modo análogo:

$$
||p_S(w)|| = ||\begin{bmatrix}
	||p_{s_1}(w)|| \\
	\dots \\
	||p_{s_k}(w)||
\end{bmatrix}||
$$

\bigskip

Para dar una intuición geométrica del hecho precedente, supongamos (sin 
pérdida de generalidad) que $S = <e_1, \dots, e_k>$ está dada por los 
primeros $k$ vectores canónicos y $w = (w_1, \dots, w_n)$. 
Claramente la proyección ortogonal de $w$ sobre $S$ es 
$(w_1, \dots, w_k)$ (las primeras $k$ componentes de $w$). Entonces 
verifiquemos la expresión anterior:

$$
||\begin{bmatrix}
	||p_{s_1}(w)|| \\	
	\dots \\
	||p_{s_k}(w)||
\end{bmatrix}|| = 
||\begin{bmatrix}
	cos(\alpha_1) \ ||w|| \\
	\dots \\
	cos(\alpha_k) \ ||w||
\end{bmatrix}|| = 
||\begin{bmatrix}
	w \ e_1 \\
	\dots \\
	w \ e_k
\end{bmatrix}|| =  
||\begin{bmatrix}
	w_1 \\
	\dots \\
	w_k
\end{bmatrix}||
$$

\bigskip

Informalmente, el caso general se puede reducir a este caso simple 
mediante una matriz rotación, que preserva las normas.

\bigskip

\underline{Metaheurísticas GRASP}

\bigskip

En el contexto del tratamiento de problemas de complejidad $NP$, el 
enfoque metaheurístico procura encontrar buenas soluciones (no 
necesariamente las mejores) en tiempo polinomial. Algunas 
metaheurísticas son:

\begin{itemize}
	\item Simulated-annealing
	\item Tabu-search
	\item GRASP
	\item Algoritmos genéticos
	\item Colonia de hormigas
\end{itemize}

Encontrar la metaheurística más adecuada depende del problema que se 
quiera resolver y debe ser contrastada empíricamente con los resultados 
obtenidos por otras metaheurísticas.

\bigskip

Específicamente una metaheurística de tipo \textit{GRASP} combina dos 
aspectos. Por un lado, requiere cierto tipo de búsqueda local, es decir: 
explorar un entorno de la solución de la iteración actual, para lo cual 
es preciso definir una noción de vecindad en el espacio; en nuestro caso 
se tratará de definir una noción de vecindad en el espacio de matrices 
de incidencia dirigida asociadas a un grafo. Y por otro lado, GRASP 
requiere cierto grado de azar en la elección del vecino a explorar en 
la siguiente iteración; esto busca impedir que el algoritmo se 
estabilice entorno a un mínimo local (no necesariamente un mínimo 
global). 

\bigskip

En la próxima sección se presentará el algoritmo y se describirán las 
particularidades de la implementación de una metaheurística GRASP en el 
contexto del problema que queremos resolver.

\bigskip

\textbf{Implementación}

\bigskip

En esta sección describiremos el algoritmo y los detalles de la 
implementación.

\bigskip

\textbf{La ``mejor" matriz de incidencia dirigida}

\bigskip

El primer paso de la estrategia que vamos a desarrollar es tratar de 
encontrar la ``mejor" matriz de incidencia dirigida, o dicho en otros 
términos: de orientar los ejes del grafo $G$ del mejor modo posible para 
minimizar el error:

$$E(x) = ||D\bm{x}-\bm{f}||^2$$
 
Como el espacio matrices de incidencia dirigida asociadas a $G$ es un 
espacio de tamaño exponencial respecto a la cantidad de ejes, dado que 
cada eje puede tener dos orientaciones, nuestra propuesta está centrada 
en construir un algoritmo que devuelva la ``mejor solución posible".

\bigskip

\underline{Preprocesamiento de las hojas del grafo}

\bigskip

Recordemos la formulación inicial del problema: queremos hallar una 
función de pesos en los nodos $x: V \rightarrow \mathbb{R}$ de modo de 
minimizar el error:

$$E(x) := \sum_{e \in E} (dx(e) - f(e))^2$$

\bigskip

Consideremos una hoja $v$ de $G$ (un nodo de grado 1), a su único 
vecino $w$ y al eje que los liga $e=(v,w)$. De acuerdo a la formulación, 
surge que $x(v) = x(w) \pm f(e)$, es decir que $x(v)$ queda 
determinado por $x(w)$, $f(e)$ y la elección de una orientación de $e$.

\bigskip

Por lo tanto, es posible reducir el grafo original $G$, eliminando 
recursivamente sus  hojas de modo de obtener una grafo más simple (un 
grafo con menos ejes y nodos):

\bigskip

\begin{verbatim}
         while (G has leaves) {
             G = stripLeaves(G);
         }
\end{verbatim}

Un corolario es que si $G$ es un árbol, entonces $x: V 
\rightarrow \mathbb{R}$ queda determinada por alguna orientación de 
los ejes y por $x(v)$ para un nodo $v$.

\bigskip

De ahora en adelante consideraremos que el grado de los nodos de $G$ es 
al menos 2.
 
\bigskip

\underline{Una base semi-ortogonal}

\bigskip

Un hecho simple que se deriva de la expresión que vincula a las matrices 
$L$ y $D$:

$$L = D^t D$$

es que el producto interno de dos columnas distintas de una matriz de 
incidencia orientada vale $-1$ o $0$ dependiendo de si existe un eje 
entre los nodos asociados a dichas columnas o no. Notar el hecho 
evidente de que si el producto es $0$, las columnas son ortogonales.

\bigskip

Ahora notemos lo siguiente: si G es conexo, el ángulo entre dos 
columnas $d_i$ y $d_j$ de $D$ está contenido en el rango 
$[\frac{\pi}{2},\frac{2\pi}{3}]$. Para ver esto, supongamos que existe 
un eje entre los nodos $v_i$ y $v_j$ asociados a dichas columnas. 
El ángulo puede medirse de este modo:

$$cos(\alpha) \ ||d_i|| \ ||d_j||= d_i  d_j \implies cos(\alpha) = 
\frac{d_i d_j}{||d_i|| \ ||d_j||}$$

El módulo de esta fracción se maximiza cuando las normas de las columnas
 son más pequeñas o de modo equivalente: cuando los grados de los nodos 
 correspondientes se minimizan. Recordemos que el grado de cada nodo de 
 $G$, luego de preprocesar las hojas, es a lo sumo 2. Supongamos que  
 $deg(v_i) = deg(v_j) = 2$. Entonces resulta que:

$$cos \ \alpha = \frac{d_i d_j}{||d_i|| \ ||d_j||} = \frac{-1}{2}$$

\bigskip

Implicando que $\alpha$ es a lo sumo $\frac{2\pi}{3}$.

\bigskip

Denotaremos base \textit{semi-ortogonal} a una base (de un 
$\mathbb{R}$-espacio vectorial) tal que el ángulo entre cualquier par de 
generadores está en el rango $[\frac{\pi}{2},\frac{2\pi}{3}]$.

\bigskip

El hecho de que las columnas de una matriz de incidencia dirigida 
definan una base semi-ortogonal va a justificar el criterio que 
emplearemos para compararlas.

\bigskip

\underline{La \textit{pseudo-norma}. Un criterio para comparar matrices 
de incidencia dirigida}

\bigskip

Una interpretación geométrica del problema $argmin_D||D \bm x- \bm f
||^2$ es la siguiente: encontrar la ``mejor" matriz de incidencia 
dirigida $D$ tal que la proyección ortogonal de $f$ sobre el subespacio 
generado por las columnas de $D$ tenga norma máxima. En el caso de que 
el vector $f$ esté contenido en el subespacio, entonces el problema 
tiene solución exacta, es decir: el sistema lineal $Dx = f$ tiene 
solución.

\bigskip

Para resolver el problema, necesitamos un criterio para comparar  
matrices de incidencia dirigida asociadas al grafo $G$. Como las 
columnas de una matriz de incidencia dirigida $D$ definen una base 
semi-ortogonal, aproximaremos la norma de la proyección 
ortogonal de $f$ sobre $D$ como si las columnas de $D$ fueran 
ortogonales y denotaremos a esta aproximación \textit{pseudo-norma}.
 De este modo, entre dos matrices de incidencia dirigida $D$ y 
 $\bar{D}$ nuestro criterio será optar por aquella con mayor 
 pseudo-norma. 

\bigskip

La pseudo-norma se puede calcular en dos pasos. El paso inicial sería 
calcular el producto interno de $f$ con las columnas de $D$:

$$
D^t f = \begin{bmatrix}
	cos(\alpha_1) \ ||d_1|| \ ||f||\\
	\dots \\
	cos(\alpha_n) \ ||d_n|| \ ||f||
\end{bmatrix}
$$

\bigskip

Y luego dividir cada componente por la norma correspondiente:

$$
pseudoNorm_f(D) = \begin{bmatrix}
	\frac{cos(\alpha_1) \ ||d_1|| \ ||f||}{||d_1||}\\
	\dots \\
	\frac{cos(\alpha_n) \ ||d_n|| \ ||f||}{||d_n||}
\end{bmatrix} = \begin{bmatrix}
	cos(\alpha_1) \ ||f||\\
	\dots \\
	cos(\alpha_n) \ ||f||
\end{bmatrix}
$$

\bigskip

Notemos que para cambiar la dirección de un eje en una matriz de 
orientación dirigida $D$, hay que invertir la fila correspondiente al 
eje que se quiere modificar. Y de modo más general, si queremos cambiar 
la dirección de varios ejes simultáneamente la nueva matriz $\bar{D}$
se puede calcular del siguiente modo:

$$\bar{D} = \bar{I} D$$

\bigskip

donde $\bar{I}$ es la matriz diagonal de $m \times m$ que tiene un $-1$ 
en cada fila asociada a cada eje que queremos invertir y $1$ en el resto 
de sus componentes.

\bigskip


Notemos que la norma de las columnas de $D$ y $\bar{D}$ son iguales 
porque sólo difieren en el signo de algunas componentes. Por lo tanto, 
la norma de las columnas de las matrices de incidencia dirigida puede 
calularse una sola vez.

\bigskip

\textbf{El enfoque metaheurístico}

\bigskip

El problema que debemos precisar ahora es cómo hacer para explorar el 
espacio de matrices de incidencia dirigida asociado a $G = (V,E)$. 

\bigskip

Recordemos que el problema que queremos resolver es el de encontrar una 
matriz de incidencia dirigida $D$ que maximice $pseudoNorm_f(D)$. 
Cabe aclarar que según nuestro entendimiento, no es posible resolverlo 
por métodos exactos porque el tamaño del espacio es $2^m$  (donde 
$|E|=m$). Este espacio es extremadamente grande incluso para grafos 
pequeños. Por lo tanto, el problema requiere un enfoque metaheurístico 
que nos permita encontrar una solución ``buena" (aunque no 
necesariamente la mejor).

\bigskip

Como ya mencionamos, las metaheurísticas \textit{GRASP} combinan azar y 
búsqueda local. La búsqueda local se refiere a la exploración de 
una vecindad de un punto del espacio. En nuestro caso el espacio es el 
conjunto de matrices de incidencia dirigida asociadas a $G$. Una 
definición obvia de vecindad de una matriz de incidencia dirigida $D$ es 
el conjunto de matrices que difieren en la dirección de un eje, o sea 
aquellas que tienen una fila invertida con respecto a $D$. 


with randomness. Local search refers to exploring some neighborhood of 
the of the current ``point". In our case a ``point" is some choice of 
directions for the edges of G, or in more practical terms: is a 
concrete matrix $D$ with 1's and -1's fixed. The neighborhood of a 
point can be defined in different ways, an obvious defintion is to 
change the direction of one edge (switching the corresponding pair of 
(1,-1) in $D$). The random part refers to choosing as the ``new" point 
to explore a random neighbor among the ``best" ones. Every time a global 
maximum is found, it replaces the previous one. The simplicity of 
\textit{GRASP} and the fact tvec it can be parallelized in a natural 
way makes it an interesting choice.

\bigskip

Our strategy is a variant of \textit{GRASP} tvec inverts the local and 
random parts: we choose random neighbors and set the best of them as 
our new point. It will be as follows:

\begin{enumerate}
	\item Initialization: an arbitrary directed incidence matrix $D$ 
	will be generated based on the labels of the nodes of $G$. Each edge 
	will be oriented from the lower label node to the higher one. 
	\item Set the parameters of the algorithm
	\begin{enumerate}
		\item $\alpha$: the number of edges of the current point to 
		modify to obtain a neighbor
		\item $\beta$: the number of neighbors to consider in each 
		iteration
	\end{enumerate}
	\item Iterate for a while. In each iteration.
	\begin{enumerate}
		\item Choose $\beta$ $\alpha$-tuples as the neighbors of the
		current point
		\item Set as the new point the ``best" neighbor, the one tvec 
		maximizes $pseudoNorm_f$
 	\end{enumerate}
	\item After some predefined number of iterations (or time lapse) a  
	global candidate $D$ is obtained. To calculate the integration 
	problem associated to $D$, solve by \textit{Conjugate Gradient} 
	the linear system:
	$$L x = D^t f$$
\end{enumerate}

\bigskip

\textbf{Implementation}

\bigskip

\emph{Some calculation remarks}

\bigskip

*To calculate $||D^t f||_2$ it is not neccessary to do it explicitly.

*Laplacian matrix implementation

\bigskip

\emph{Interesting facts}

\bigskip

For dense-edge graphs, the chance of improving the maximization 
condition for a random neighbor decreases with an increasing $\alpha$. 
This fact was noted while adjusting parameters for an input mid-size 
complete graph (100 nodes). 

\bigskip

To illustrate the situation, consider a matrix $D$ and a neighbor 
$\bar{D}$. As mentioned above they are related via some diagonal matrix 
$\bar{I}$ with values $1,-1$ in the diagonal:

$$\bar{D} = \bar{I} D$$

Note tvec the maximization condition for the neighbor $\bar{D}$ uncovers 
a new point of view of the problem:

$$||\bar{D}^tf||_2 = ||(\bar{I} D)^t f||_2 = ||D^t (\bar{I} f)||_2 = 
||D^t \bar{f}||_2$$

Our problem can be stated as: find the ``closest" point $\bar{f}$ to 
the subspace generated by $D$. Where $f$ and $\bar{f}$, differ in the 
sign of exactly $\alpha$ components.

\bigskip

So informally, as the span of $D$ has dimension $n-1$, and as $\alpha$ 
increases, any random point $\bar{f}$ will tend to diverge from tvec 
subspace in dense-edge graphs ($m \approx O(n^2)$).

\bigskip

\emph{Parameter adjustment}

\bigskip

La función \textbf{Initialize(G)} se encarga de construir 
una matriz de incidencia dirigida inicial. Concretamente la orientación 
de los ejes de esa matriz se define desde el nodo con menor 
etiqueta al de mayor a partir de un etiquetado previo de los nodos del 
grafo. 

\smallskip

La función \textbf{Neighbors(currentPoint)} devuelve un conjunto de 
vecinos del punto que se está explorando en la iteración. El conjunto 
de vecinos se arma en base a los parámetro $\alpha$ y $\beta$. El 
resultado es un conjunto de $\beta$ vecinos cada uno de los cuales 
difiere en la dirección de exactamente $\alpha$ ejes con respecto 
$currentPoint$.

\smallskip

La función $BestNeighbor(\Omega,f)$ devuelve el mejor vecino del conjunto 
de entrada. O sea, aquel que tiene mayor $pseudo-norma$. Y por último, 
la función $PN(bestNeighbor,f)$ calcula la $pseudo-norma$ de la matriz 
de incidencia de entrada.

\end{document}
