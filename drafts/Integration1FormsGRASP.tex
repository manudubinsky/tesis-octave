\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}

\usepackage[table,xcdraw]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{ulem}
\usepackage{color}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage {tikz}
\usetikzlibrary{arrows,matrix,positioning,fit}
%\usepackage{bera}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{framed}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
%\setenumerate[2]{label=\roman*.}
\usepackage{blindtext}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{hhline}
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor {processblue}{cmyk}{0.96,0,0,0}

%\setenumerate[1]{label=\textbf{\thesection.\arabic*.}}


\begin{document}
\textbf{Integration of 1-forms on Graphs}

\bigskip

\textbf{Introduction}

\bigskip

Given a connected graph $G=(V,E)$ with $V$ vertices, and $E$ edges and a 
1-form $v: E \rightarrow \mathbb{R}$. We're looking for the 0-form 
$x: V \rightarrow \mathbb{R}$ minimizing the error:

$$E(x) = \sum_{(i,j) \in E} ||dx_{ij} - v_{ij}||^2$$

where $dx: E \rightarrow \mathbb{R}$ is the differential of $x$.

\bigskip


\textbf{Obs.} (Abuses of notation): We use $V$ to denote both the set 
of vertices and the total number of vertices of the graph. And we denote 
$v$ to the 1-form and $v_i$ is the $i-th$ vertex.

\bigskip

\textbf{Properties of the oriented incidence matrix $D$ and other facts}

\bigskip

\underline{Oriented incidence matrix $D$}

\bigskip

We can rewrite the minimization problem in matrix form as follows:

$$E(x) = \sum_{(i,j) \in E} ||dx_{ij} - v_{ij}||^2 = || D x - v||^2$$

Where $D$ is the "best" \emph{oriented incidence matrix}. 

\bigskip

\underline{The problem is equivalent to solve a linear system}

\bigskip

To solve the minimizatiom problem is equivalent to find where the 
gradient of $E(x)$ vanishes:
 
$$\nabla E = [\frac{\partial E}{\partial x_1}, \dots, \frac{\partial 
E}{\partial x_n}] = D^tDx-D^tv=0$$

It is equivalent to solve the linear system:

$$D^tDx = D^tv$$

As the rank of $D$ is $n-1$, the linear system may not have a solution. 
But solving the problem by iterative methods will converge to the 
closest ``possible" guess.

\bigskip

\underline{Relation to the \textit{Laplacian} matrix and rank of $D$}

\bigskip

The directed incidence matrix has the following property:

$$L = D^t D$$

where $L$ is the \textit{Laplacian} matrix of $G$. The rank of 
$L$ is: $n-c$ (where $c$ is the number of connected components of $G$). 
In our case, as $G$ is connected, the rank of $L$ (and consequently the 
rank of $D$) is $n-1$.

\bigskip

\underline{The orthogonal complement of the \textit{Laplacian} matrix}

\bigskip

The \textit{Laplacian} matrix is simmetric. In our case its rank is $n-1$ 
because $G$ is connected. So its orthogonal complement is the same 
subspace as its kernel. More precisely:

$$ker(L) = arg_{<x>} (L x = 0) = <(1,\dots,1)^t>$$

And the 1-dimensional subspace $L^{\bot}$:

$$L^{\bot} = arg_{<x>} (L^t x = 0) = arg_{<x>} (L x = 0) = <(1,\dots,1)^t>$$

So the equation $L x = D^t D x = D^t v = w$ will have a solution if $w 
\in <L>$. In other words if $w$ is as "orthogonal as possible" to 
$L^{\bot}$. This fact can be expressed as follows:

$$(1,\dots,1) w = cos(\alpha) ||(1,\dots,1)^t||^2 ||w||^2 = n \ cos(\alpha) 
||w||^2$$

It follows that:

$$\frac{|\sum w_i|}{||w||^2} = n |cos(\alpha)|$$

So the original minimization problem can be expressed as:

$$argmin_w(\frac{|\sum w_i|}{||w||^2})$$

Where $w = D^t v$, which means that we must find the "best" $D$: the 
"best" directions for the edges of $G$.

\bigskip


\textbf{Exact and metaheuristic approaches}

\bigskip

The exact approach finds the best solution. The algorithms are not 
suitable for big datasets. But they may be the best choice for small 
ones.

\bigskip

The metaheuristic approach tries to find good solutions (not 
necessarilly the best) in polynomial time. Some metaheuristics are:

\begin{itemize}
	\item Simulated-annealing
	\item Tabu-search
	\item GRASP
	\item Genetic algorithms
	\item Ant-colony
\end{itemize}

Finding the best metaheuristic depends on the problem and must be 
demonstrated empirically.

\bigskip

\underline{A GRASP metaheuristic for our problem}

\bigskip

Recall from the last section that the problem that we want to solve is 
the following:

$$argmin_w(\frac{|\sum w_i|}{||w||^2})$$

where $w = D^t v$. Trying to solve it by exact methods is impossible 
because there are $2^m$ choices for $D$ (i.e. there are $2^m$ directed 
graphs). This is an extremely large space even for small graphs. So a 
metaheuristic approach is the alternative to find a best-effort solution.

\bigskip

A \textit{GRASP} metaheuristic combines some local search 
with randomness. Local search refers to exploring some neighborhood of 
the of the current "point". In our case a "point" is some choice of 
directions for the edges of G, or in more practical terms: is a 
concrete matrix $D$ with 1's and -1's fixed. The neighborhood of a 
point can be defined in different ways, an obvious defintion is to 
change the direction of one edge (switching the corresponding pair of 
(1,-1) in $D$). The random part refers to choosing as the "new" point to 
explore a random neighbor among the "best" ones. Every time a global 
minimum is found, it replaces the previous one. The simplicity of 
\textit{GRASP} and the fact that it can be parallelized in a natural 
way makes it an interesting choice.

\bigskip

Our strategy is a variant of \textit{GRASP} that inverts the local and 
random parts: we choose random neighbors and set the best of them as 
our new point. It will be as follows:

\begin{enumerate}
	\item Initialization: an arbitrary directed incidence matrix $D$ 
	will be generated based on the labels of the nodes of $G$. Each edge 
	will be oriented from the lower label node to the higher one. 
	\item Iterate for a while. In each iteration.
	\begin{enumerate}
		\item Choose a random number ($\alpha$) of edges to modify (a 
		number in some parametrized range, i.e. an integer in [1,5])
		\item Choose a random number ($\beta$) of $\alpha$-tuples 
		(another number in some parametrized range)
		\item Choose $\beta$ $\alpha$-tuples as the neighbors of the
		current point
		\item Set as the new point the "best" neighbor, the one that 
		minimizes:
		$$argmin_w(\frac{|\sum w_i|}{||w||^2})$$
 	\end{enumerate}
	\item After some predefined number of iterations (or time lapse) a  
	global candidate $w = D^t v$ is obtained. To calculate the 
	integration problem associated to $w$, we solve by 
	\textit{conjugate gradient} the linear system:
	$$L x = w$$
\end{enumerate}

\bigskip

\emph{Some calculation remarks}

\bigskip

As pointed above, we choose the "best" neighbor: the one that minimizes:

$$argmin_w(\frac{|\sum w_i|}{||w||^2})$$

Note that changing the direction of an edge (switching the corresponding 
pair of (1,-1) in the current $D$ matrix), does not alter the 
numerator. And the new denominator can be calculated based on the value 
of the current point by simply subtracting and adding the appropriate 
compenents. So finding the best neighbor is a fast operation.

\end{document}
