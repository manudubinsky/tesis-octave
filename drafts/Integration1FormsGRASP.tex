\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}

\usepackage[table,xcdraw]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{ulem}
\usepackage{color}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage {tikz}
\usetikzlibrary{arrows,matrix,positioning,fit}
%\usepackage{bera}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{framed}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
%\setenumerate[2]{label=\roman*.}
\usepackage{blindtext}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{hhline}
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor {processblue}{cmyk}{0.96,0,0,0}

%\setenumerate[1]{label=\textbf{\thesection.\arabic*.}}


\begin{document}
\textbf{Integration of 1-forms on Graphs}

\bigskip

\textbf{Introduction}

\bigskip

Given a connected graph $G=(V,E)$ and a 1-form $f: E \rightarrow 
\mathbb{R}$. We're looking for the 0-form $x: V \rightarrow \mathbb{R}$ 
minimizing the error:

$$E(x) = \sum_{(i,j) \in E} ||dx_{ij} - f_{ij}||^2$$

where $dx: E \rightarrow \mathbb{R}$ is the differential of $x$.

\bigskip

The differential $dx$ determine an orientation of the edges of $G$. 
The problem can be expressed informally in two steps:

\begin{enumerate}
\item Find the ``best" orientation of the edges of $G$
\item Solve the corresponding linear system
\end{enumerate}

The minimization expression on the oriented version of $G$ is: 

$$E(x) = \sum_{e=v_i \rightarrow v_j \in E} ||(x(v_j) - x(v_i)) - f_{ij}||^2$$

\bigskip

The \textit{oriented incidence matrix} of an oriented graph $G$ is the $m 
\times n$ matrix $D$ such that for each oriented edge $e_k=v_i \rightarrow 
v_j$: $D_{ki} = -1$, $D_{kj} = 1$ and $0$ otherwise.

\bigskip

In more practical terms, step 1) can be restated as: find the "best" 
\textit{oriented incidence matrix} $D$ such that: 

$$||Dx-f||^2$$

achives the global minimum.

\bigskip

As we describe below, step 2) can be solved by some iterative method for 
linear systems.

\bigskip

\textbf{Properties of the oriented incidence matrix $D$ and other facts}

\bigskip

\underline{Relation to the \textit{Laplacian} matrix and rank of $D$}

\bigskip

The directed incidence matrix has the following property:

$$L = D^t D$$

where $L$ is the \textit{Laplacian} matrix of $G$. The rank of 
$L$ is: $n-c$ (where $c$ is the number of connected components of $G$). 
In our case, as $G$ is connected, the rank of $L$ (and consequently the 
rank of $D$) is $n-1$.

\bigskip

\underline{Solving the linear system induced by $D$}

\bigskip

To solve step 2), that is: once we have a candidate matrix $D$, we must 
find the 0-form $x: V \rightarrow \mathbb{R}$. The minimizatiom problem 
is equivalent to find where the gradient of $E(x)$ vanishes:
 
$$\nabla E = [\frac{\partial E}{\partial x_1}, \dots, \frac{\partial 
E}{\partial x_n}] = D^tDx-D^tv=0$$

Which in turn is equivalent to solve the linear system:

$$D^tDx = Lx = D^tv$$

As the rank of $D$ is $n-1$, the linear system may not have a solution. 
But solving the problem by iterative methods will converge to the 
closest ``possible" guess. In particular, $L$ meets the hypothesis of 
the \textit{Conjugate Gradient} method.

\bigskip

\textbf{The ``best" oriented incidence matrix $D$}

\bigskip

In order to solve step 1) we need some criteria to compare the possible 
orientations of the edges of $G$. Note that given an orientation, and its 
corresponding $D$ matrix, changing the direction of a single edge is 
reflected as inverting the row corresponding to the edge. More generally, 
if we want to change the direction of several edges the new incidence 
matrix $\bar{D}$ can be computed as follows:

$$\bar{D} = \bar{I} D$$

\bigskip

where $\bar{I}$ is the $m \times m$ diagonal matrix that has a $-1$ in 
every row associated to each of the edges that we need to invert an $1$ 
elsewhere.

\bigskip

A geometric interpretation of the problem: 

$$argmin_D||Dx-f||^2$$

\bigskip

would be: find the ``best" $D$ so that the \textit{orthogonal 
projection} of $f$ onto the subspace generated by the columns of $D$ is 
maximum w.r.t. to its norm. 

\bigskip

Note that the expression:

\begin{equation}
     D^t f = \begin{bmatrix}
         cos(\alpha_1) ||d_1||_2 ||f||_2\\
         \dots \\
         cos(\alpha_n) ||d_n||_2 ||f||_2
	\end{bmatrix}
\end{equation}

where $\alpha_i$ is the angle among $f$ and the $i$-th column of $D$ 
expresses the orthogonal projection of $f$ onto the subspace generated 
by $D$. As matrices $D$ and $\bar{D}$ differ in the signs of some 
entries, the norm of the respective columns are the same:

\begin{equation}
     \bar{D}^t f = \begin{bmatrix}
         cos(\bar{\alpha_1}) ||d_1||_2 ||f||_2\\
         \dots \\
         cos(\bar{\alpha_n}) ||d_n||_2 ||f||_2
	\end{bmatrix}
\end{equation}

So in our context, the ``best" directed incidence matrix $D$ will be the 
one that maximizes $||D^t f||_2$. $D$ will not be unique in general.

\bigskip

\textbf{Metaheuristic approach}

\bigskip

The problem that should be addressed is how to explore the space of 
directed incidence matrices associated to $D$. It is an intractable 
combinatorial problem in general. 

\bigskip

The metaheuristic approach tries to find good solutions (not 
necessarilly the best) in polynomial time. Some metaheuristics are:

\begin{itemize}
	\item Simulated-annealing
	\item Tabu-search
	\item GRASP
	\item Genetic algorithms
	\item Ant-colony
\end{itemize}

Finding the best metaheuristic depends on the problem and must be 
demonstrated empirically.

\bigskip

\underline{A GRASP metaheuristic}

\bigskip

Recall from the last section that the problem that we want to solve is 
to find a directed incidence matrix $D$ that maximizes $||D^t f||_2$ on
the space of all directed incidence matrices associated to $G$. Trying 
to solve it by exact methods (i.e. backtracking) is impossible because 
there are $2^m$ choices for $D$ (i.e. there are $2^m$ directed graphs). 
This is an extremely large space even for small graphs. So a 
metaheuristic approach is the alternative to find a best-effort 
solution.

\bigskip

A \textit{GRASP} metaheuristic combines some local search 
with randomness. Local search refers to exploring some neighborhood of 
the of the current ``point". In our case a ``point" is some choice of 
directions for the edges of G, or in more practical terms: is a 
concrete matrix $D$ with 1's and -1's fixed. The neighborhood of a 
point can be defined in different ways, an obvious defintion is to 
change the direction of one edge (switching the corresponding pair of 
(1,-1) in $D$). The random part refers to choosing as the ``new" point 
to explore a random neighbor among the ``best" ones. Every time a global 
maximum is found, it replaces the previous one. The simplicity of 
\textit{GRASP} and the fact that it can be parallelized in a natural 
way makes it an interesting choice.

\bigskip

Our strategy is a variant of \textit{GRASP} that inverts the local and 
random parts: we choose random neighbors and set the best of them as 
our new point. It will be as follows:

\begin{enumerate}
	\item Initialization: an arbitrary directed incidence matrix $D$ 
	will be generated based on the labels of the nodes of $G$. Each edge 
	will be oriented from the lower label node to the higher one. 
	\item Set the parameters of the algorithm
	\begin{enumerate}
		\item $\alpha$: the number of edges of the current point to 
		modify to obtain a neighbor
		\item $\beta$: the number of neighbors to consider in each 
		iteration
	\end{enumerate}
	\item Iterate for a while. In each iteration.
	\begin{enumerate}
		\item Choose $\beta$ $\alpha$-tuples as the neighbors of the
		current point
		\item Set as the new point the ``best" neighbor, the one that 
		maximizes:
		$$||D^t f||_2$$
 	\end{enumerate}
	\item After some predefined number of iterations (or time lapse) a  
	global candidate $D$ is obtained. To calculate the integration 
	problem associated to $D$, solve by \textit{Conjugate Gradient} 
	the linear system:
	$$L x = D^t f$$
\end{enumerate}

\bigskip

\textbf{Implementation}

\bigskip

\emph{Some calculation remarks}

\bigskip

*To calculate $||D^t f||_2$ it is not neccessary to do it explicitly.

*Laplacian matrix implementation

\bigskip

\emph{Interesting facts}

\bigskip

For dense-edge graphs, the chance of improving the maximization 
condition for a random neighbor decreases with an increasing $\alpha$. 
This fact was noted while adjusting parameters for an input mid-size 
complete graph (100 nodes). 

\bigskip

To illustrate the situation, consider a matrix $D$ and a neighbor 
$\bar{D}$. As mentioned above they are related via some diagonal matrix 
$\bar{I}$ with values $1,-1$ in the diagonal:

$$\bar{D} = \bar{I} D$$

Note that the maximization condition for the neighbor $\bar{D}$ uncovers 
a new point of view of the problem:

$$||\bar{D}^tf||_2 = ||(\bar{I} D)^t f||_2 = ||D^t (\bar{I} f)||_2 = 
||D^t \bar{f}||_2$$

Our problem can be stated as: find the ``closest" point $\bar{f}$ to 
the subspace generated by $D$. Where $f$ and $\bar{f}$, differ in the 
sign of exactly $\alpha$ components.

\bigskip

So informally, as the span of $D$ has dimension $n-1$, and as $\alpha$ 
increases, any random point $\bar{f}$ will tend to diverge from that 
subspace in dense-edge graphs ($m \approx O(n^2)$).

\bigskip

\emph{Parameter adjustment}

\bigskip

\emph{Varios}

\bigskip

Almost Orthogonal Basis. The inner product of two columns of any oriented 
incidence matrix is $-1$ or $0$ depending if there exists an edge among
the corresponding nodes or not. Note that if the inner product is $0$, 
the columns are orthogonal.

\bigskip

If G is connected, the angle among two columns $d_i$ and $d_j$ of $D$ is 
at least $45^{\circ}$. To see this, suppose that there exists an edge 
between the corresponding nodes $v_i$ and $v_j$. The angle can be 
measured in the following way:

$$cos \alpha ||d_i|| ||d_j||= d_i  d_j$$

So: 

$$cos \alpha = \frac{d_i d_j}{||d_i|| ||d_j||}$$

The module of this fraction is maximized when the norms of the columns 
are minimal or equivalently when the degrees of the nodes are minimal. So 
(as $G$ is connected) suppose that $deg(v_i) = 1$ and $deg(v_j)=2$. Then 
we have:

$$cos \alpha = \frac{d_i d_j}{||d_i|| ||d_j||} = \frac{-1}{\sqrt{2}}$$

\bigskip

Implying that $\alpha$ is at least $45^{\circ}$.

\bigskip
Another related fact is that for any dimension of the space 
$\mathbb{R}^m$ only two columns of $D$ meet that dimension, which 
is a geometric way of expressing that the dimension of $D$ is at least 
$n-1$ (we already showed via the Laplacian matrix that its dimension is 
in fact $n-1$)

\bigskip

So the reason that the angles among the columns of $D$ are at least 
$45^{\circ}$ and at most $90^{\circ}$ justifies the criteria that we 
propose to compare the possible choices of oriented incidence matrices.
The criteria that we'll adopt is to 

\end{document}
